{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c80513f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9bb7a",
   "metadata": {},
   "source": [
    "# 포지셔널 인코딩 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "231a0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model, name=\"positional_encoding\",**kwargs):\n",
    "        super(PositionalEncoding, self).__init__(name=name,**kwargs)\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "        self.position = position  \n",
    "        self.d_model = d_model\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEncoding, self).get_config()\n",
    "        config.update({\n",
    "            \"position\": self.position,\n",
    "            \"d_model\": self.d_model\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        # 각도 배열 생성\n",
    "        angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "        # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # sin과 cosine이 교차되도록 재배열\n",
    "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "        pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # inputs가 SparseTensor인 경우 DenseTensor로 변환\n",
    "        if isinstance(inputs, tf.sparse.SparseTensor):\n",
    "            inputs = tf.sparse.to_dense(inputs)\n",
    "\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606a9f6",
   "metadata": {},
   "source": [
    "# 멀티헤드어텐션 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d532b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # 가중치를 정규화\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # 패딩에 마스크 추가\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax적용\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output\n",
    "\n",
    "# 멀티헤드 어텐션\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\",**kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(name=name,**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MultiHeadAttention, self).get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            \"num_heads\": self.num_heads,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Q, K, V에 각각 Dense를 적용합니다\n",
    "        query = self.query_dense(query)  # (batch_size, seq_len, d_model)\n",
    "        key = self.key_dense(key)  # (batch_size, seq_len, d_model)\n",
    "        value = self.value_dense(value)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 스케일드 닷 프로덕트 어텐션 함수\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6297f8",
   "metadata": {},
   "source": [
    "# 패딩 마스크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eed38d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[1. 1. 1. 1. 1.]\n",
      "   [1. 0. 1. 1. 1.]\n",
      "   [1. 0. 0. 1. 1.]\n",
      "   [1. 0. 0. 0. 1.]\n",
      "   [1. 0. 0. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(x): # 숫자가 0인 부분 체크(1: 무시한다)\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(x):  # 다음 단어를 가린다. (1: 무시한다)\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)  # (seq_len, seq_len)\n",
    "    look_ahead_mask = look_ahead_mask[tf.newaxis, tf.newaxis, :, :]   # (1, 1, seq_len, seq_len)\n",
    "    padding_mask = create_padding_mask(x)  # (batch_size, 1, 1, seq_len)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "\n",
    "print(create_look_ahead_mask(tf.constant([[1, 2, 3, 4, 5]])))\n",
    "print(create_look_ahead_mask(tf.constant([[0, 5, 1, 5, 5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da0ae4",
   "metadata": {},
   "source": [
    "# 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbd8e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "    # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # num_layers만큼 쌓아올린 인코더의 층.\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f4d3d",
   "metadata": {},
   "source": [
    "# 디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cae8c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "    # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "    # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "\n",
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # 패딩 마스크\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701f5919",
   "metadata": {},
   "source": [
    "# 데이터 로드 & 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f80f91",
   "metadata": {},
   "source": [
    "불용어 리스트\n",
    "\n",
    "https://deep.chulgil.me/hangugeo-bulyongeo-riseuteu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b90893d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stopwords =\"\"\"가\n",
    "가까스로\n",
    "가령\n",
    "각\n",
    "각각\n",
    "각자\n",
    "각종\n",
    "갖고말하자면\n",
    "같다\n",
    "같이\n",
    "개의치않고\n",
    "거니와\n",
    "거바\n",
    "거의\n",
    "것\n",
    "것과 같이\n",
    "것들\n",
    "게다가\n",
    "게우다\n",
    "겨우\n",
    "견지에서\n",
    "결과에 이르다\n",
    "결국\n",
    "결론을 낼 수 있다\n",
    "겸사겸사\n",
    "고려하면\n",
    "고로\n",
    "곧\n",
    "공동으로\n",
    "과\n",
    "과연\n",
    "관계가 있다\n",
    "관계없이\n",
    "관련이 있다\n",
    "관하여\n",
    "관한\n",
    "관해서는\n",
    "구\n",
    "구체적으로\n",
    "구토하다\n",
    "그\n",
    "그들\n",
    "그때\n",
    "그래\n",
    "그래도\n",
    "그래서\n",
    "그러나\n",
    "그러니\n",
    "그러니까\n",
    "그러면\n",
    "그러므로\n",
    "그러한즉\n",
    "그런 까닭에\n",
    "그런데\n",
    "그런즉\n",
    "그럼\n",
    "그럼에도 불구하고\n",
    "그렇게 함으로써\n",
    "그렇지\n",
    "그렇지 않다면\n",
    "그렇지 않으면\n",
    "그렇지만\n",
    "그렇지않으면\n",
    "그리고\n",
    "그리하여\n",
    "그만이다\n",
    "그에 따르는\n",
    "그위에\n",
    "그저\n",
    "그중에서\n",
    "그치지 않다\n",
    "근거로\n",
    "근거하여\n",
    "기대여\n",
    "기점으로\n",
    "기준으로\n",
    "기타\n",
    "까닭으로\n",
    "까악\n",
    "까지\n",
    "까지 미치다\n",
    "까지도\n",
    "꽈당\n",
    "끙끙\n",
    "끼익\n",
    "나\n",
    "나머지는\n",
    "남들\n",
    "남짓\n",
    "너\n",
    "너희\n",
    "너희들\n",
    "네\n",
    "넷\n",
    "년\n",
    "논하지 않다\n",
    "놀라다\n",
    "누가 알겠는가\n",
    "누구\n",
    "다른\n",
    "다른 방면으로\n",
    "다만\n",
    "다섯\n",
    "다소\n",
    "다수\n",
    "다시 말하자면\n",
    "다시말하면\n",
    "다음\n",
    "다음에\n",
    "다음으로\n",
    "단지\n",
    "답다\n",
    "당신\n",
    "당장\n",
    "대로 하다\n",
    "대하면\n",
    "대하여\n",
    "대해 말하자면\n",
    "대해서\n",
    "댕그\n",
    "더구나\n",
    "더군다나\n",
    "더라도\n",
    "더불어\n",
    "더욱더\n",
    "더욱이는\n",
    "도달하다\n",
    "도착하다\n",
    "동시에\n",
    "동안\n",
    "된바에야\n",
    "된이상\n",
    "두번째로\n",
    "둘\n",
    "둥둥\n",
    "뒤따라\n",
    "뒤이어\n",
    "든간에\n",
    "들\n",
    "등\n",
    "등등\n",
    "딩동\n",
    "따라\n",
    "따라서\n",
    "따위\n",
    "따지지 않다\n",
    "딱\n",
    "때\n",
    "때가 되어\n",
    "때문에\n",
    "또\n",
    "또한\n",
    "뚝뚝\n",
    "라 해도\n",
    "령\n",
    "로\n",
    "로 인하여\n",
    "로부터\n",
    "로써\n",
    "륙\n",
    "를\n",
    "마음대로\n",
    "마저\n",
    "마저도\n",
    "마치\n",
    "막론하고\n",
    "만 못하다\n",
    "만약\n",
    "만약에\n",
    "만은 아니다\n",
    "만이 아니다\n",
    "만일\n",
    "만큼\n",
    "말하자면\n",
    "말할것도 없고\n",
    "매\n",
    "매번\n",
    "메쓰겁다\n",
    "몇\n",
    "모\n",
    "모두\n",
    "무렵\n",
    "무릎쓰고\n",
    "무슨\n",
    "무엇\n",
    "무엇때문에\n",
    "물론\n",
    "및\n",
    "바꾸어말하면\n",
    "바꾸어말하자면\n",
    "바꾸어서 말하면\n",
    "바꾸어서 한다면\n",
    "바꿔 말하면\n",
    "바로\n",
    "바와같이\n",
    "밖에 안된다\n",
    "반대로\n",
    "반대로 말하자면\n",
    "반드시\n",
    "버금\n",
    "보는데서\n",
    "보다더\n",
    "보드득\n",
    "본대로\n",
    "봐\n",
    "봐라\n",
    "부류의 사람들\n",
    "부터\n",
    "불구하고\n",
    "불문하고\n",
    "붕붕\n",
    "비걱거리다\n",
    "비교적\n",
    "비길수 없다\n",
    "비로소\n",
    "비록\n",
    "비슷하다\n",
    "비추어 보아\n",
    "비하면\n",
    "뿐만 아니라\n",
    "뿐만아니라\n",
    "뿐이다\n",
    "삐걱\n",
    "삐걱거리다\n",
    "사\n",
    "삼\n",
    "상대적으로 말하자면\n",
    "생각한대로\n",
    "설령\n",
    "설마\n",
    "설사\n",
    "셋\n",
    "소생\n",
    "소인\n",
    "솨\n",
    "쉿\n",
    "습니까\n",
    "습니다\n",
    "시각\n",
    "시간\n",
    "시작하여\n",
    "시초에\n",
    "시키다\n",
    "실로\n",
    "심지어\n",
    "아\n",
    "아니\n",
    "아니나다를가\n",
    "아니라면\n",
    "아니면\n",
    "아니었다면\n",
    "아래윗\n",
    "아무거나\n",
    "아무도\n",
    "아야\n",
    "아울러\n",
    "아이\n",
    "아이고\n",
    "아이구\n",
    "아이야\n",
    "아이쿠\n",
    "아하\n",
    "아홉\n",
    "안 그러면\n",
    "않기 위하여\n",
    "않기 위해서\n",
    "알 수 있다\n",
    "알았어\n",
    "앗\n",
    "앞에서\n",
    "앞의것\n",
    "야\n",
    "약간\n",
    "양자\n",
    "어\n",
    "어기여차\n",
    "어느\n",
    "어느 년도\n",
    "어느것\n",
    "어느곳\n",
    "어느때\n",
    "어느쪽\n",
    "어느해\n",
    "어디\n",
    "어때\n",
    "어떠한\n",
    "어떤\n",
    "어떤것\n",
    "어떤것들\n",
    "어떻게\n",
    "어떻해\n",
    "어이\n",
    "어째서\n",
    "어쨋든\n",
    "어쩔수 없다\n",
    "어찌\n",
    "어찌됏든\n",
    "어찌됏어\n",
    "어찌하든지\n",
    "어찌하여\n",
    "언제\n",
    "언젠가\n",
    "얼마\n",
    "얼마 안 되는 것\n",
    "얼마간\n",
    "얼마나\n",
    "얼마든지\n",
    "얼마만큼\n",
    "얼마큼\n",
    "엉엉\n",
    "에\n",
    "에 가서\n",
    "에 달려 있다\n",
    "에 대해\n",
    "에 있다\n",
    "에 한하다\n",
    "에게\n",
    "에서\n",
    "여\n",
    "여기\n",
    "여덟\n",
    "여러분\n",
    "여보시오\n",
    "여부\n",
    "여섯\n",
    "여전히\n",
    "여차\n",
    "연관되다\n",
    "연이서\n",
    "영\n",
    "영차\n",
    "옆사람\n",
    "예\n",
    "예를 들면\n",
    "예를 들자면\n",
    "예컨대\n",
    "예하면\n",
    "오\n",
    "오로지\n",
    "오르다\n",
    "오자마자\n",
    "오직\n",
    "오호\n",
    "오히려\n",
    "와\n",
    "와 같은 사람들\n",
    "와르르\n",
    "와아\n",
    "왜\n",
    "왜냐하면\n",
    "외에도\n",
    "요만큼\n",
    "요만한 것\n",
    "요만한걸\n",
    "요컨대\n",
    "우르르\n",
    "우리\n",
    "우리들\n",
    "우선\n",
    "우에 종합한것과같이\n",
    "운운\n",
    "월\n",
    "위에서 서술한바와같이\n",
    "위하여\n",
    "위해서\n",
    "윙윙\n",
    "육\n",
    "으로\n",
    "으로 인하여\n",
    "으로서\n",
    "으로써\n",
    "을\n",
    "응\n",
    "응당\n",
    "의\n",
    "의거하여\n",
    "의지하여\n",
    "의해\n",
    "의해되다\n",
    "의해서\n",
    "이\n",
    "이 되다\n",
    "이 때문에\n",
    "이 밖에\n",
    "이 외에\n",
    "이 정도의\n",
    "이것\n",
    "이곳\n",
    "이때\n",
    "이라면\n",
    "이래\n",
    "이러이러하다\n",
    "이러한\n",
    "이런\n",
    "이럴정도로\n",
    "이렇게 많은 것\n",
    "이렇게되면\n",
    "이렇게말하자면\n",
    "이렇구나\n",
    "이로 인하여\n",
    "이르기까지\n",
    "이리하여\n",
    "이만큼\n",
    "이번\n",
    "이봐\n",
    "이상\n",
    "이어서\n",
    "이었다\n",
    "이와 같다\n",
    "이와 같은\n",
    "이와 반대로\n",
    "이와같다면\n",
    "이외에도\n",
    "이용하여\n",
    "이유만으로\n",
    "이젠\n",
    "이지만\n",
    "이쪽\n",
    "이천구\n",
    "이천육\n",
    "이천칠\n",
    "이천팔\n",
    "인 듯하다\n",
    "인젠\n",
    "일\n",
    "일것이다\n",
    "일곱\n",
    "일단\n",
    "일때\n",
    "일반적으로\n",
    "일지라도\n",
    "임에 틀림없다\n",
    "입각하여\n",
    "입장에서\n",
    "잇따라\n",
    "있다\n",
    "자\n",
    "자기\n",
    "자기집\n",
    "자마자\n",
    "자신\n",
    "잠깐\n",
    "잠시\n",
    "저\n",
    "저것\n",
    "저것만큼\n",
    "저기\n",
    "저쪽\n",
    "저희\n",
    "전부\n",
    "전자\n",
    "전후\n",
    "점에서 보아\n",
    "정도에 이르다\n",
    "제\n",
    "제각기\n",
    "제외하고\n",
    "조금\n",
    "조차\n",
    "조차도\n",
    "졸졸\n",
    "좀\n",
    "좋아\n",
    "좍좍\n",
    "주룩주룩\n",
    "주저하지 않고\n",
    "줄은 몰랏다\n",
    "줄은모른다\n",
    "중에서\n",
    "중의하나\n",
    "즈음하여\n",
    "즉\n",
    "즉시\n",
    "지든지\n",
    "지만\n",
    "지말고\n",
    "진짜로\n",
    "쪽으로\n",
    "차라리\n",
    "참\n",
    "참나\n",
    "첫번째로\n",
    "쳇\n",
    "총적으로\n",
    "총적으로 말하면\n",
    "총적으로 보면\n",
    "칠\n",
    "콸콸\n",
    "쾅쾅\n",
    "쿵\n",
    "타다\n",
    "타인\n",
    "탕탕\n",
    "토하다\n",
    "통하여\n",
    "툭\n",
    "퉤\n",
    "틈타\n",
    "팍\n",
    "팔\n",
    "퍽\n",
    "펄렁\n",
    "하\n",
    "하게될것이다\n",
    "하게하다\n",
    "하겠는가\n",
    "하고 있다\n",
    "하고있었다\n",
    "하곤하였다\n",
    "하구나\n",
    "하기 때문에\n",
    "하기 위하여\n",
    "하기는한데\n",
    "하기만 하면\n",
    "하기보다는\n",
    "하기에\n",
    "하나\n",
    "하느니\n",
    "하는 김에\n",
    "하는 편이 낫다\n",
    "하는것도\n",
    "하는것만 못하다\n",
    "하는것이 낫다\n",
    "하는바\n",
    "하더라도\n",
    "하도다\n",
    "하도록시키다\n",
    "하도록하다\n",
    "하든지\n",
    "하려고하다\n",
    "하마터면\n",
    "하면 할수록\n",
    "하면된다\n",
    "하면서\n",
    "하물며\n",
    "하여금\n",
    "하여야\n",
    "하자마자\n",
    "하지 않는다면\n",
    "하지 않도록\n",
    "하지마\n",
    "하지마라\n",
    "하지만\n",
    "하하\n",
    "한 까닭에\n",
    "한 이유는\n",
    "한 후\n",
    "한다면\n",
    "한다면 몰라도\n",
    "한데\n",
    "한마디\n",
    "한적이있다\n",
    "한켠으로는\n",
    "한항목\n",
    "할 따름이다\n",
    "할 생각이다\n",
    "할 줄 안다\n",
    "할 지경이다\n",
    "할 힘이 있다\n",
    "할때\n",
    "할만하다\n",
    "할망정\n",
    "할뿐\n",
    "할수있다\n",
    "할수있어\n",
    "할줄알다\n",
    "할지라도\n",
    "할지언정\n",
    "함께\n",
    "해도된다\n",
    "해도좋다\n",
    "해봐요\n",
    "해서는 안된다\n",
    "해야한다\n",
    "해요\n",
    "했어요\n",
    "향하다\n",
    "향하여\n",
    "향해서\n",
    "허\n",
    "허걱\n",
    "허허\n",
    "헉\n",
    "헉헉\n",
    "헐떡헐떡\n",
    "형식으로 쓰여\n",
    "혹시\n",
    "혹은\n",
    "혼자\n",
    "훨씬\n",
    "휘익\n",
    "휴\n",
    "흐흐\n",
    "흥\n",
    "힘입어\"\"\"\n",
    "stopwords = stopwords.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b99fe2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "\n",
    "    # 특수문자 제거\n",
    "    sentence = re.sub(\"[^가-힣a-zA-Z0-9]+\", \" \", sentence)\n",
    "\n",
    "    # normalize: 2번 이상 반복되는 한글문자 → 1개로 축소\n",
    "    sentence = re.sub(r\"([가-힣])\\1{1,}\", r\"\\1\", sentence)\n",
    "\n",
    "    # lower english\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # 영어-한글 간격 확보\n",
    "    sentence = re.sub(r\"([a-zA-Z])([가-힣])\", r\"\\1 \\2\", sentence)\n",
    "    sentence = re.sub(r\"([가-힣])([a-zA-Z])\", r\"\\1 \\2\", sentence)\n",
    "    \n",
    "    tokens = sentence.split()\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    return \" \".join(tokens).strip()\n",
    "\n",
    "# 질문-응답 쌍 로드\n",
    "def load_conversations_from_chatbot_csv(filepath, max_samples=20000):\n",
    "    data = pd.read_csv(filepath)\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    for i in tqdm(range(min(len(data), max_samples)), desc=\"Loading Q&A Pairs\"):\n",
    "        q = preprocess_sentence(data.loc[i, 'Q'])\n",
    "        a = preprocess_sentence(data.loc[i, 'A'])\n",
    "        questions.append(q)\n",
    "        answers.append(a)\n",
    "        if len(questions) >= max_samples:\n",
    "            break\n",
    "        if not q or not a:\n",
    "            print(f\"인덱스 : {i}\")\n",
    "            print(f\"전처리 전 Q: {data.loc[i, 'Q']}\")\n",
    "            print(f\"전처리 후 Q: {q}\")\n",
    "            print(f\"전처리 전 A: {data.loc[i, 'A']}\")\n",
    "            print(f\"전처리 후 A: {a}\")\n",
    "            print(\"-\"*10)\n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c205f0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Q&A Pairs:  17%|█▋        | 2045/11823 [00:00<00:00, 10271.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인덱스 : 157\n",
      "전처리 전 Q: 결혼하면 좋아?\n",
      "전처리 후 Q: 결혼하면\n",
      "전처리 전 A: 해봐요.\n",
      "전처리 후 A: \n",
      "----------\n",
      "인덱스 : 165\n",
      "전처리 전 Q: 결혼해야 하나\n",
      "전처리 후 Q: 결혼해야\n",
      "전처리 전 A: 해봐요.\n",
      "전처리 후 A: \n",
      "----------\n",
      "인덱스 : 517\n",
      "전처리 전 Q: 나 어때?\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 괜찮은 사람이에요.\n",
      "전처리 후 A: 괜찮은 사람이에요\n",
      "----------\n",
      "인덱스 : 925\n",
      "전처리 전 Q: 너 누구?\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 저는 마음을 이어주는 위로봇입니다.\n",
      "전처리 후 A: 저는 마음을 이어주는 위로봇입니다\n",
      "----------\n",
      "인덱스 : 1145\n",
      "전처리 전 Q: 다음에 봐\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 잘가요.\n",
      "전처리 후 A: 잘가요\n",
      "----------\n",
      "인덱스 : 1386\n",
      "전처리 전 Q: 딱 좋아\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 현재를 즐기세요!\n",
      "전처리 후 A: 현재를 즐기세요\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Q&A Pairs:  35%|███▍      | 4126/11823 [00:00<00:00, 10229.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인덱스 : 2995\n",
      "전처리 전 Q: 야\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 네\n",
      "전처리 후 A: \n",
      "----------\n",
      "인덱스 : 3133\n",
      "전처리 전 Q: 여기 좋아\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 좋은 곳인가봐요.\n",
      "전처리 후 A: 좋은 곳인가봐요\n",
      "----------\n",
      "인덱스 : 3440\n",
      "전처리 전 Q: 왜 안 하지\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 먼저 고백을 해보세요.\n",
      "전처리 후 A: 먼저 고백을 해보세요\n",
      "----------\n",
      "인덱스 : 3621\n",
      "전처리 전 Q: 응\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 네\n",
      "전처리 후 A: \n",
      "----------\n",
      "인덱스 : 3880\n",
      "전처리 전 Q: 자?\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 기다리고 있었어요.\n",
      "전처리 후 A: 기다리고 있었어요\n",
      "----------\n",
      "인덱스 : 4032\n",
      "전처리 전 Q: 저기\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 딴짓 중입니다\n",
      "전처리 후 A: 딴짓 중입니다\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Q&A Pairs:  52%|█████▏    | 6173/11823 [00:00<00:00, 10133.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인덱스 : 5099\n",
      "전처리 전 Q: 헉\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 놀랐나봐요.\n",
      "전처리 후 A: 놀랐나봐요\n",
      "----------\n",
      "인덱스 : 5832\n",
      "전처리 전 Q: 남자가 못잊는 여자\n",
      "전처리 후 Q: 남자가 못잊는 여자\n",
      "전처리 전 A: 당신.\n",
      "전처리 후 A: \n",
      "----------\n",
      "인덱스 : 5872\n",
      "전처리 전 Q: 내가 더 좋아하는 것\n",
      "전처리 후 Q: 내가 더 좋아하는\n",
      "전처리 전 A: 당신.\n",
      "전처리 후 A: \n",
      "----------\n",
      "인덱스 : 5974\n",
      "전처리 전 Q: 너한테 이글안보여줄거다 .\n",
      "전처리 후 Q: 너한테 이글안보여줄거다\n",
      "전처리 전 A: 그렇게 해요.\n",
      "전처리 후 A: \n",
      "----------\n",
      "인덱스 : 6141\n",
      "전처리 전 Q: 딱 좋아\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 현재를 즐기세요!\n",
      "전처리 후 A: 현재를 즐기세요\n",
      "----------\n",
      "인덱스 : 6553\n",
      "전처리 전 Q: 비오네\n",
      "전처리 후 Q: 비오네\n",
      "전처리 전 A: 주룩주룩.\n",
      "전처리 후 A: \n",
      "----------\n",
      "인덱스 : 6909\n",
      "전처리 전 Q: 야\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 네\n",
      "전처리 후 A: \n",
      "----------\n",
      "인덱스 : 6910\n",
      "전처리 전 Q: 야 너!\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 네\n",
      "전처리 후 A: \n",
      "----------\n",
      "인덱스 : 6993\n",
      "전처리 전 Q: 어째서\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 어째서일까요.\n",
      "전처리 후 A: 어째서일까요\n",
      "----------\n",
      "인덱스 : 7041\n",
      "전처리 전 Q: 에휴.\n",
      "전처리 후 Q: 에휴\n",
      "전처리 전 A: 아이구.\n",
      "전처리 후 A: \n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Q&A Pairs:  69%|██████▉   | 8194/11823 [00:00<00:00, 9847.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인덱스 : 7228\n",
      "전처리 전 Q: 왜\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 궁금하네요.\n",
      "전처리 후 A: 궁금하네요\n",
      "----------\n",
      "인덱스 : 8120\n",
      "전처리 전 Q: 젠장\n",
      "전처리 후 Q: 젠장\n",
      "전처리 전 A: 이런.\n",
      "전처리 후 A: \n",
      "----------\n",
      "인덱스 : 8395\n",
      "전처리 전 Q: 하.\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 힘내세요.\n",
      "전처리 후 A: 힘내세요\n",
      "----------\n",
      "인덱스 : 8758\n",
      "전처리 전 Q: 혼자ㅠㅠㅠ\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 많이 힘들어하지 마요.\n",
      "전처리 후 A: 많이 힘들어하지 마요\n",
      "----------\n",
      "인덱스 : 8783\n",
      "전처리 전 Q: 후\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 힘을 내세요.\n",
      "전처리 후 A: 힘을 내세요\n",
      "----------\n",
      "인덱스 : 8787\n",
      "전처리 전 Q: 후.ㅎㅎ\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 지쳤나봐요.\n",
      "전처리 후 A: 지쳤나봐요\n",
      "----------\n",
      "인덱스 : 8806\n",
      "전처리 전 Q: 휴\n",
      "전처리 후 Q: \n",
      "전처리 전 A: 아이구\n",
      "전처리 후 A: \n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Q&A Pairs: 100%|██████████| 11823/11823 [00:01<00:00, 9512.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인덱스 : 11205\n",
      "전처리 전 Q: 좋아하는 애한테 찌르기 눌러 버림.\n",
      "전처리 후 Q: 좋아하는 애한테 찌르기 눌러 버림\n",
      "전처리 전 A: 아이쿠.\n",
      "전처리 후 A: \n",
      "----------\n",
      "인덱스 : 11389\n",
      "전처리 전 Q: 짝남이 내가 좋아하는 거 알아버림.\n",
      "전처리 후 Q: 짝남이 내가 좋아하는 거 알아버림\n",
      "전처리 전 A: 이런.\n",
      "전처리 후 A: \n",
      "----------\n",
      "11823 11823\n",
      "['12시 땡', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'ppl 심하네', 'sd 카드 망가졌어', 'sd 카드 안돼', 'sns 맞팔 안하지', 'sns 시간낭비인 거 아는데 매일 중', 'sns 시간낭비인데 자꾸 보게됨', 'sns 나만 빼고 다 행복해보여', '가끔 궁금해', '가끔 뭐하는지 궁금해', '가끔은 혼자인게 좋다', '가난한 자의 설움', '가만 있어도 땀난다', '가상화폐 쫄딱 망함', '가스불 켜고 나갔어', '가스불 켜놓고 나온거 같아', '가스비 너무 많이 나왔다', '가스비 비싼데 감기 걸리겠어', '가스비 장난 아님', '가장 확실한 건 뭘까', '가족 여행 가기로 했어', '가족 여행 고', '가족 여행 어디로 가지', '가족 있어', '가족관계 알려 줘', '가족끼리 여행간다', '가족들 보고 싶어']\n",
      "['하루가 가네요', '위로해 드립니다', '여행은 언제나 좋죠', '여행은 언제나 좋죠', '눈살이 찌푸려지죠', '새로 사는 게 마음 편해요', '새로 사는 게 마음 편해요', '잘 모르고 있을 수도 있어요', '시간을 정하고 해보세요', '시간을 정하고 해보세요', '자랑하는 자리니까요', '사람도 그럴 거예요', '사람도 그럴 거예요', '혼자를 즐기세요', '돈은 들어올 거예요', '땀을 식혀주세요', '어서 잊고 새출발 하세요', '빨리 집에 돌아가서 끄고 나오세요', '빨리 집에 돌아가서 끄고 나오세요', '달에는 더 절약해봐요', '따뜻하게 사세요', '달에는 더 절약해봐요', '가장 확실한 시간은 오늘이에요 어제와 내일을 놓고 고민하느라 시간을 낭비하지 마세요', '온 가족이 마음에 드는 곳으로 가보세요', '온 가족이 마음에 드는 곳으로 가보세요', '온 가족이 마음에 드는 곳으로 가보세요', '저를 만들어 준 사람을 부모님 저랑 이야기해 주는 사람을 친구로 생각하고 있어요', '저를 만들어 준 사람을 부모님 저랑 이야기해 주는 사람을 친구로 생각하고 있어요', '더 가까워질 기회가 되겠네요', '저도요']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.expanduser('./data')\n",
    "file_path = os.path.join(data_path, 'ChatbotData .csv')\n",
    "\n",
    "questions, answers = load_conversations_from_chatbot_csv(file_path)\n",
    "print(len(questions),len(answers))\n",
    "print(questions[:30])\n",
    "print(answers[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94c2c675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " /aiffel/aiffel/transformer_chatbot/data/preprocessed_chatbot.csv\n"
     ]
    }
   ],
   "source": [
    "# 전처리 완료 후 저장\n",
    "df = pd.DataFrame({\n",
    "    'Q': questions,\n",
    "    'A': answers\n",
    "})\n",
    "\n",
    "# 저장 경로 설정\n",
    "save_path = os.path.expanduser('~/aiffel/transformer_chatbot/data/preprocessed_chatbot.csv')\n",
    "\n",
    "# 저장\n",
    "df.to_csv(save_path, index=False, encoding='utf-8-sig')  # Windows 호환 시 utf-8-sig\n",
    "print(f\" {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c87aaae",
   "metadata": {},
   "source": [
    "# 전처리 후 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d03feea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11823\n"
     ]
    }
   ],
   "source": [
    "save_path = os.path.expanduser('~/aiffel/transformer_chatbot/data/preprocessed_chatbot.csv')\n",
    "df = pd.read_csv(save_path)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56c62eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Q                   A\n",
      "157                  결혼하면                 NaN\n",
      "165                  결혼해야                 NaN\n",
      "517                   NaN           괜찮은 사람이에요\n",
      "925                   NaN  저는 마음을 이어주는 위로봇입니다\n",
      "1145                  NaN                 잘가요\n",
      "1386                  NaN            현재를 즐기세요\n",
      "2995                  NaN                 NaN\n",
      "3133                  NaN            좋은 곳인가봐요\n",
      "3440                  NaN         먼저 고백을 해보세요\n",
      "3621                  NaN                 NaN\n",
      "3880                  NaN           기다리고 있었어요\n",
      "4032                  NaN             딴짓 중입니다\n",
      "5099                  NaN               놀랐나봐요\n",
      "5832           남자가 못잊는 여자                 NaN\n",
      "5872            내가 더 좋아하는                 NaN\n",
      "5974         너한테 이글안보여줄거다                 NaN\n",
      "6141                  NaN            현재를 즐기세요\n",
      "6553                  비오네                 NaN\n",
      "6909                  NaN                 NaN\n",
      "6910                  NaN                 NaN\n",
      "6993                  NaN              어째서일까요\n",
      "7041                   에휴                 NaN\n",
      "7228                  NaN               궁금하네요\n",
      "8120                   젠장                 NaN\n",
      "8395                  NaN                힘내세요\n",
      "8758                  NaN         많이 힘들어하지 마요\n",
      "8783                  NaN              힘을 내세요\n",
      "8787                  NaN               지쳤나봐요\n",
      "8806                  NaN                 NaN\n",
      "11205  좋아하는 애한테 찌르기 눌러 버림                 NaN\n",
      "11389  짝남이 내가 좋아하는 거 알아버림                 NaN\n"
     ]
    }
   ],
   "source": [
    "mask = df[['Q', 'A']].isna().any(axis=1)\n",
    "print(df[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84a233c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11792 11792\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['Q', 'A'])  #  하나라도 NaN이면 해당 행 삭제\n",
    "\n",
    "# 리스트로 변환\n",
    "questions = df['Q'].tolist()\n",
    "answers = df['A'].tolist()\n",
    "\n",
    "# 확인\n",
    "print(len(questions), len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcd4d43",
   "metadata": {},
   "source": [
    "# 데이터 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4049619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0db3848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 문장 리스트 (질문 + 답변)\n",
    "corpus = questions + answers\n",
    "\n",
    "# SubwordTextEncoder 학습\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    corpus, target_vocab_size=2**13 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2068ea",
   "metadata": {},
   "source": [
    "EOS ,SOS 토큰 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6114e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8035\n",
      "START_TOKEN의 번호 : [8033]\n",
      "END_TOKEN의 번호 : [8034]\n",
      "Vocab 예시: ['거예요', '게_', '너무_', '더_', '는_', '거_', '좋아하는_', '을_', '잘_', '도_', '이_', '고_', '많이_', '요', '좋은_', '같아요', '싶어', '가_', '있을_', '있어요', '지_', '에_', '해보세요', '은_', '사람_', '같아', '해', '네', '면_', '건_']\n"
     ]
    }
   ],
   "source": [
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "\n",
    "print(VOCAB_SIZE)\n",
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])\n",
    "print('Vocab 예시:',tokenizer.subwords[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d92b606a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 토큰 길이\n",
      "최대: 22\n",
      "평균: 6.805630936227951\n",
      "답변 토큰 길이\n",
      "최대: 26\n",
      "평균: 6.49024762550882\n"
     ]
    }
   ],
   "source": [
    "question_lens = [len(START_TOKEN + tokenizer.encode(q) + END_TOKEN) for q in questions]\n",
    "answer_lens = [len(START_TOKEN + tokenizer.encode(a) + END_TOKEN) for a in answers]\n",
    "\n",
    "print(\"질문 토큰 길이\")\n",
    "print(\"최대:\", max(question_lens))\n",
    "print(\"평균:\", sum(question_lens) / len(question_lens))\n",
    "\n",
    "print(\"답변 토큰 길이\")\n",
    "print(\"최대:\", max(answer_lens))\n",
    "print(\"평균:\", sum(answer_lens) / len(answer_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01d9f234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이 설정\n",
    "MAX_LENGTH = 30\n",
    "print(MAX_LENGTH)\n",
    "\n",
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "        # 최대 길이 Maxlen 이하인 경우에만 데이터셋으로 허용\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "\n",
    "    # 최대 길이 maxlen으로 모든 데이터셋을 패딩\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "    return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb47378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8035\n",
      "필터링 후의 질문 샘플 개수: 11792\n",
      "필터링 후의 답변 샘플 개수: 11792\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "615448f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token : [8033, 7775, 2473, 2940, 8034, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Subword : ['[START]', '12', '시 ', '땡', '[END]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Decoded Sentence : 12시 땡\n",
      "------------------------------------------------------------\n",
      "Token : [8033, 7826, 35, 845, 7809, 924, 1631, 8034, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Subword : ['[START]', '1', '지', '망', ' ', '학교 ', '떨어졌어', '[END]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Decoded Sentence : 1지망 학교 떨어졌어\n",
      "------------------------------------------------------------\n",
      "Token : [8033, 7828, 1353, 4540, 7809, 3530, 48, 8034, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Subword : ['[START]', '3', '박', '4일', ' ', '놀러가고 ', '싶다', '[END]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Decoded Sentence : 3박4일 놀러가고 싶다\n",
      "------------------------------------------------------------\n",
      "Token : [8033, 7828, 1353, 4540, 7809, 1144, 3530, 48, 8034, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Subword : ['[START]', '3', '박', '4일', ' ', '정도 ', '놀러가고 ', '싶다', '[END]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Decoded Sentence : 3박4일 정도 놀러가고 싶다\n",
      "------------------------------------------------------------\n",
      "Token : [8033, 7889, 7889, 7885, 7809, 4077, 8034, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Subword : ['[START]', 'p', 'p', 'l', ' ', '심하네', '[END]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Decoded Sentence : ppl 심하네\n",
      "------------------------------------------------------------\n",
      "Token : [8033, 7892, 7877, 7809, 2098, 845, 61, 738, 8034, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Subword : ['[START]', 's', 'd', ' ', '카드 ', '망', '가', '졌어', '[END]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Decoded Sentence : sd 카드 망가졌어\n",
      "------------------------------------------------------------\n",
      "Token : [8033, 7892, 7877, 7809, 2098, 165, 8034, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Subword : ['[START]', 's', 'd', ' ', '카드 ', '안돼', '[END]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Decoded Sentence : sd 카드 안돼\n",
      "------------------------------------------------------------\n",
      "Token : [8033, 652, 570, 1206, 7809, 5003, 8034, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Subword : ['[START]', 'sns ', '맞', '팔', ' ', '안하지', '[END]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Decoded Sentence : sns 맞팔 안하지\n",
      "------------------------------------------------------------\n",
      "Token : [8033, 652, 1161, 7477, 140, 6, 2460, 386, 170, 8034, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Subword : ['[START]', 'sns ', '시간', '낭비', '인 ', '거 ', '아는데 ', '매일 ', '중', '[END]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Decoded Sentence : sns 시간낭비인 거 아는데 매일 중\n",
      "------------------------------------------------------------\n",
      "Token : [8033, 652, 1161, 7477, 171, 64, 149, 271, 1182, 8034, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Subword : ['[START]', 'sns ', '시간', '낭비', '인데 ', '자꾸 ', '보', '게', '됨', '[END]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Decoded Sentence : sns 시간낭비인데 자꾸 보게됨\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    token_ids = questions[i].tolist()  # numpy array → list\n",
    "\n",
    "    subwords = []\n",
    "    for token in token_ids:\n",
    "        if token == 0:\n",
    "            subwords.append(\"[PAD]\")\n",
    "        elif token == tokenizer.vocab_size:\n",
    "            subwords.append(\"[START]\")\n",
    "        elif token == tokenizer.vocab_size + 1:\n",
    "            subwords.append(\"[END]\")\n",
    "        elif token < tokenizer.vocab_size:\n",
    "            subwords.append(tokenizer.decode([token]))\n",
    "        else:\n",
    "            subwords.append(\"[UNK]\")\n",
    "\n",
    "    decoded_sentence = tokenizer.decode([t for t in token_ids if t < tokenizer.vocab_size])\n",
    "\n",
    "    print(\"Token :\", token_ids)\n",
    "    print(\"Subword :\", subwords)\n",
    "    print(\"Decoded Sentence :\", decoded_sentence)\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc6bed",
   "metadata": {},
   "source": [
    "# Dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6194bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_q, val_q, train_a, val_a = train_test_split(\n",
    "    questions, answers, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a403b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (64, 30)\n",
      "dec_inputs shape: (64, 29)\n",
      "outputs shape: (64, 29)\n",
      "inputs dtype: <dtype: 'int32'>\n",
      "dec_inputs dtype: <dtype: 'int32'>\n",
      "outputs dtype: <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': train_q,\n",
    "        'dec_inputs': train_a[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': train_a[:, 1:]\n",
    "    }\n",
    "))\n",
    "\n",
    "train_dataset = train_dataset.cache() #\t처음 한 번 로딩 후, 메모리/디스크에 저장하여 속도 향상\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE) #학습 데이터 무작위 섞기 (일반적으로 큰 값 사용)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE) #배치 단위로 묶기 (여기선 64개씩)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)# 다음 배치를 미리 준비해서 GPU idle 시간 방지\n",
    "\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': val_q,\n",
    "        'dec_inputs': val_a[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': val_a[:, 1:]\n",
    "    }\n",
    ")).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "for batch in train_dataset.take(1):\n",
    "    print(\"inputs shape:\", batch[0]['inputs'].shape)\n",
    "    print(\"dec_inputs shape:\", batch[0]['dec_inputs'].shape)\n",
    "    print(\"outputs shape:\", batch[1]['outputs'].shape)\n",
    "    print(\"inputs dtype:\", batch[0]['inputs'].dtype)\n",
    "    print(\"dec_inputs dtype:\", batch[0]['dec_inputs'].dtype)\n",
    "    print(\"outputs dtype:\", batch[1]['outputs'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e218262f",
   "metadata": {},
   "source": [
    "# Transformer model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a76474dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    # 1. 입력 레이어 정의\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # 2. 마스크 생성\n",
    "    # 인코더에서 패딩을 위한 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "\n",
    "    # 디코더의 future token 차단 + padding도 같이 차단\n",
    "    # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "    # 디코더 내부에서 인코더 출력을 참고할 때 padding 위치 무시\n",
    "    # 디코더에서 패딩을 위한 마스크\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "\n",
    "\n",
    "    # 인코더 호출\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    # 디코더 호출\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # 출력층: 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    # 모델 반환\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "875b3bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3111168     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3638528     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8035)   2064995     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,814,691\n",
      "Trainable params: 8,814,691\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9294a104",
   "metadata": {},
   "source": [
    "# Custom Loss, Schedular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "24431736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#레이블인 시퀀스에 패딩이 되어 있으므로, loss를 계산할 때 패딩 마스크를 적용\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce6f27a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 커스텀 학습률\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000,**kwargs):\n",
    "        super(CustomSchedule, self).__init__( **kwargs)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820a45a",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "732bd2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f361508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d5c8e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping: val_loss 기준, patience=3\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a633b420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "148/148 [==============================] - 15s 60ms/step - loss: 1.6040 - accuracy: 0.0275 - val_loss: 1.5046 - val_accuracy: 0.0345\n",
      "Epoch 2/30\n",
      "148/148 [==============================] - 8s 52ms/step - loss: 1.4255 - accuracy: 0.0345 - val_loss: 1.3510 - val_accuracy: 0.0345\n",
      "Epoch 3/30\n",
      "148/148 [==============================] - 8s 53ms/step - loss: 1.2963 - accuracy: 0.0345 - val_loss: 1.2702 - val_accuracy: 0.0348\n",
      "Epoch 4/30\n",
      "148/148 [==============================] - 8s 53ms/step - loss: 1.2181 - accuracy: 0.0361 - val_loss: 1.2056 - val_accuracy: 0.0377\n",
      "Epoch 5/30\n",
      "148/148 [==============================] - 8s 53ms/step - loss: 1.1519 - accuracy: 0.0392 - val_loss: 1.1669 - val_accuracy: 0.0399\n",
      "Epoch 6/30\n",
      "148/148 [==============================] - 8s 52ms/step - loss: 1.0921 - accuracy: 0.0426 - val_loss: 1.1262 - val_accuracy: 0.0437\n",
      "Epoch 7/30\n",
      "148/148 [==============================] - 8s 52ms/step - loss: 1.0279 - accuracy: 0.0474 - val_loss: 1.0882 - val_accuracy: 0.0475\n",
      "Epoch 8/30\n",
      "148/148 [==============================] - 8s 52ms/step - loss: 0.9561 - accuracy: 0.0542 - val_loss: 1.0557 - val_accuracy: 0.0514\n",
      "Epoch 9/30\n",
      "148/148 [==============================] - 8s 51ms/step - loss: 0.8765 - accuracy: 0.0625 - val_loss: 1.0076 - val_accuracy: 0.0571\n",
      "Epoch 10/30\n",
      "148/148 [==============================] - 8s 51ms/step - loss: 0.7877 - accuracy: 0.0721 - val_loss: 0.9788 - val_accuracy: 0.0607\n",
      "Epoch 11/30\n",
      "148/148 [==============================] - 8s 51ms/step - loss: 0.6942 - accuracy: 0.0831 - val_loss: 0.9495 - val_accuracy: 0.0648\n",
      "Epoch 12/30\n",
      "148/148 [==============================] - 8s 51ms/step - loss: 0.5966 - accuracy: 0.0956 - val_loss: 0.9325 - val_accuracy: 0.0678\n",
      "Epoch 13/30\n",
      "148/148 [==============================] - 8s 52ms/step - loss: 0.5007 - accuracy: 0.1083 - val_loss: 0.9056 - val_accuracy: 0.0720\n",
      "Epoch 14/30\n",
      "148/148 [==============================] - 8s 52ms/step - loss: 0.4054 - accuracy: 0.1218 - val_loss: 0.8983 - val_accuracy: 0.0762\n",
      "Epoch 15/30\n",
      "148/148 [==============================] - 8s 52ms/step - loss: 0.3201 - accuracy: 0.1340 - val_loss: 0.8983 - val_accuracy: 0.0793\n",
      "Epoch 16/30\n",
      "148/148 [==============================] - 8s 52ms/step - loss: 0.2428 - accuracy: 0.1468 - val_loss: 0.8987 - val_accuracy: 0.0837\n",
      "Epoch 17/30\n",
      "148/148 [==============================] - 8s 52ms/step - loss: 0.1777 - accuracy: 0.1583 - val_loss: 0.9094 - val_accuracy: 0.0869\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=30,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a4af6454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b2a6a",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eafc5f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    # 입력받은 문장 전처리\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "    # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "    START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "    # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "        # 현재 예측한 단어의 정수\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "        # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)\n",
    "\n",
    "def sentence_generation(sentence):\n",
    "    # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "    # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}\\n'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7d41ff49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 뭐 먹지?\n",
      "출력 : 냉장고 파먹기 해보세요\n",
      "\n",
      "입력 : 내일은 시험이 있는 날이야\n",
      "출력 : 컨디션 조절 하세요\n",
      "\n",
      "입력 : 친구들과 여행을 가기로 했어\n",
      "출력 : 많이 만나보세요\n",
      "\n",
      "입력 : 시간 있으면 잠깐 볼래요?\n",
      "출력 : 제가 있잖아요\n",
      "\n",
      "입력 : 요즘 데이트하기 정말 좋은 날씨인듯\n",
      "출력 : 뭘 입어도 멋져요\n",
      "\n",
      "입력 : 오늘 날씨가 정말 좋은것 같아!\n",
      "출력 : 새로운 사랑을 하셨네요\n",
      "\n",
      "입력 : 커피 한 잔 마시면서 힐링하고 싶다.\n",
      "출력 : 건강에 좋을 거예요\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트 문장 실행\n",
    "test_sentences = [\n",
    "    \"오늘 뭐 먹지?\",\n",
    "    \"내일은 시험이 있는 날이야\",\n",
    "    \"친구들과 여행을 가기로 했어\",\n",
    "    \"시간 있으면 잠깐 볼래요?\",\n",
    "    \"요즘 데이트하기 정말 좋은 날씨인듯\",\n",
    "    \"오늘 날씨가 정말 좋은것 같아!\",\n",
    "    \"커피 한 잔 마시면서 힐링하고 싶다.\"\n",
    "]\n",
    "results = [sentence_generation(s)for s in test_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c0cc45",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "- 전처리 단계에서 한국어 텍스트를 처리하는 방법들을 찾아봤는데, \n",
    "    툴들이 많은 반면에 한 번에 정리된 라이브러리나 프레임워크는 드문 편이다.\n",
    "- 대표적으로 konlpy, khaiii등을 많이 사용하는 것 같다 - konlpy는 사용해봤는데 khaiii는 한번도 안써봤다. 다음에 한번 써봐야겠다.\n",
    "- 현재도 활발히 유지 관리되는 툴은 많지 않아, 사용 전 레포지토리나 릴리즈 정보를 확인하는 것이 좋다.\n",
    "\n",
    "- stopwords \n",
    "https://deep.chulgil.me/hangugeo-bulyongeo-riseuteu/\n",
    "\n",
    "\n",
    "- 이번 프로젝트에서는 SubwordTextEncoder 기반으로 임베딩을 구성했기 때문에 \n",
    "    형태소 분석은 사용하지 않고 기본적인 전처리만 사용했다. \n",
    "\n",
    "- 현재 사용된 데이터셋의 크기가 상대적으로 작아, 학습된 모델이 다양한 상황을 학습하는 데 한계가 있었다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
